import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.tree import ExtraTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
import pickle
import time

# Load and explore the dataset
crop = pd.read_csv('/mnt/data/crop_recommendation_dataset_5000.csv')
print(crop.shape)
print(crop.info())
print(crop.isnull().sum())
print(crop.duplicated().sum())
print(crop.describe())

# Value counts for the target variable
print(crop['label'].value_counts())

# Encode the crop labels to numerical values
crop_dict = {
    'rice': 1, 'maize': 2, 'jute': 3, 'cotton': 4, 'coconut': 5, 'papaya': 6,
    'orange': 7, 'apple': 8, 'muskmelon': 9, 'watermelon': 10, 'grapes': 11, 'mango': 12,
    'banana': 13, 'pomegranate': 14, 'lentil': 15, 'blackgram': 16, 'mungbean': 17,
    'mothbeans': 18, 'pigeonpeas': 19, 'kidneybeans': 20, 'chickpea': 21, 'coffee': 22
}
crop['crop_num'] = crop['label'].map(crop_dict)
crop.drop(['label'], axis=1, inplace=True)

# Features and target
X = crop.drop(['crop_num'], axis=1)
y = crop['crop_num']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Data scaling
minmax_scaler = MinMaxScaler()
standard_scaler = StandardScaler()

X_train_minmax = minmax_scaler.fit_transform(X_train)
X_test_minmax = minmax_scaler.transform(X_test)

X_train_standard = standard_scaler.fit_transform(X_train)
X_test_standard = standard_scaler.transform(X_test)

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(),
    'Naive Bayes': GaussianNB(),
    'Support Vector Machine': SVC(probability=True),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'Bagging': BaggingClassifier(),
    'AdaBoost': AdaBoostClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'Extra Trees': ExtraTreeClassifier(),
}

# Train and evaluate models using MinMaxScaler
for name, model in models.items():
    start_time = time.time()
    
    model.fit(X_train_minmax, y_train)
    y_pred = model.predict(X_test_minmax)
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')
    conf_matrix = confusion_matrix(y_test, y_pred)
    
    end_time = time.time()
    execution_time = end_time - start_time
    
    print(f"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}")
    print("Confusion Matrix:\n", conf_matrix)
    print(f"Execution Time: {execution_time:.2f} seconds")
    print("==========================================================")

# Train the final Decision Tree model with MinMaxScaler
dtc = DecisionTreeClassifier()
dtc.fit(X_train_minmax, y_train)
y_pred_dtc = dtc.predict(X_test_minmax)
print(f"Decision Tree Accuracy: {accuracy_score(y_test, y_pred_dtc):.4f}")

# Recommendation function
def recommendation(N, P, K, temperature, humidity, ph, rainfall):
    features = np.array([[N, P, K, temperature, humidity, ph, rainfall]])
    transformed_features = minmax_scaler.transform(features)  # Use MinMaxScaler
    prediction = dtc.predict(transformed_features)
    return prediction[0]

# New inputs for recommendation
N = 40
P = 50
K = 50
temperature = 40.0
humidity = 20
ph = 100
rainfall = 100
predict = recommendation(N, P, K, temperature, humidity, ph, rainfall)

# Crop recommendation mapping
crop_dict = {1: "Rice", 2: "Maize", 3: "Jute", 4: "Cotton", 5: "Coconut", 6: "Papaya", 7: "Orange",
             8: "Apple", 9: "Muskmelon", 10: "Watermelon", 11: "Grapes", 12: "Mango", 13: "Banana",
             14: "Pomegranate", 15: "Lentil", 16: "Blackgram", 17: "Mungbean", 18: "Mothbeans",
             19: "Pigeonpeas", 20: "Kidneybeans", 21: "Chickpea", 22: "Coffee"}

if predict in crop_dict:
    crop = crop_dict[predict]
    print(f"{crop} is the best crop to be cultivated")
else:
    print("Unable to recommend a proper crop for this environment.")

# Save the models and scalers
pickle.dump(dtc, open('model.pkl', 'wb'))  # Saving Decision Tree model
pickle.dump(minmax_scaler, open('minmaxscaler.pkl', 'wb'))  # Saving MinMax Scaler
pickle.dump(standard_scaler, open('standard_scaler.pkl', 'wb'))  # Saving Standard Scaler
